{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8036b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Category: rec.sport.hockey\n",
      "Document:\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, naive_bayes\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "data = newsgroups.data\n",
    "target = newsgroups.target\n",
    "\n",
    "categories = newsgroups.target_names\n",
    "print(f\"Categories: {categories}\")\n",
    "\n",
    "print(f\"Category: {categories[target[0]]}\")\n",
    "print(f\"Document:\\n{data[0]}\")\n",
    "\n",
    "def tokenize(text):\n",
    "    ttx = text.split()\n",
    " \n",
    "    return ttx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125e27a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data,target,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f57e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokens=[]\n",
    "for i in x_train:\n",
    "    list_tokens.append(tokenize(i))\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "list_tokens = [[word for word in sublist if word.lower() not in stop_words and word.isalnum()==True] for sublist in list_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33749fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2213"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_word_frequency(texts):\n",
    "    word_frequency = {}\n",
    "    for text in texts:\n",
    "        for word in text:  \n",
    "            if word in word_frequency:\n",
    "                word_frequency[word] += 1\n",
    "            else:\n",
    "                word_frequency[word] = 1\n",
    "    return word_frequency\n",
    "\n",
    "dic = count_word_frequency(list_tokens)\n",
    "dic = {k:v for (k,v) in dic.items() if v>80}\n",
    "dd = list(dic.keys())\n",
    "len(dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e993ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40212"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "list_tokens_test = []\n",
    "for text in x_test:\n",
    "    list_tokens_test.append(tokenize(text))\n",
    "\n",
    "list_tokens_test = [[word for word in sublist if word.lower() not in stop_words and word.isalnum()==True] for sublist in list_tokens_test]\n",
    "\n",
    "\n",
    "dic_test = count_word_frequency(list_tokens_test)\n",
    "len(dic_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44484f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2213"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_test = {k: v for (k, v) in dic_test.items() if v > 26}\n",
    "for i in range(11):\n",
    "    min_key = min(dic_test, key=dic_test.get)\n",
    "    del dic_test[min_key]\n",
    "len(dic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27becf3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50506d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(y_train),len(dic)))\n",
    "XX = np.zeros((len(y_test), len(dic_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3620bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_test=dd\n",
    "for i in range(len(x_train)):  \n",
    "    s = x_train[i] \n",
    "    \n",
    "    for j in range(len(dic)):  \n",
    "        X[i][j] = s.count(dd[j]) \n",
    "        \n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    s = x_test[i]\n",
    "    for j in range(len(dic_test)):\n",
    "        XX[i][j] = s.count(dd_test[j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b9a0d",
   "metadata": {},
   "source": [
    "# USING SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ec1e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = naive_bayes.MultinomialNB()\n",
    "clf.fit(X,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d5e5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[119,   1,   1,   0,   0,   1,   1,   3,   7,   7,   0,   4,   1,\n",
       "          1,   0,  21,   5,   3,   3,  21],\n",
       "       [  4, 121,   1,  13,  14,  16,   6,   4,   6,   9,   5,  13,  12,\n",
       "          2,   9,   1,   3,   0,   0,   3],\n",
       "       [  7,  32,   5,  83,  21,  51,   2,   6,   5,   9,   2,  13,  13,\n",
       "          3,   1,   3,   1,   0,   5,   1],\n",
       "       [  3,   6,   0, 143,  49,   0,  14,   6,   0,   7,   6,   2,  19,\n",
       "          1,   1,   0,   0,   1,   3,   1],\n",
       "       [  1,   7,   2,  25, 126,   1,  16,   7,   5,   8,   2,   5,  22,\n",
       "          3,   1,   0,   1,   0,   1,   1],\n",
       "       [  1,  41,   0,   7,   5, 139,   8,   2,   0,   1,   3,   6,   5,\n",
       "          3,   3,   1,   1,   0,   2,   2],\n",
       "       [  0,   4,   1,  11,  11,   1, 160,  18,   6,   3,   6,   7,  20,\n",
       "          1,   3,   0,   0,   1,   1,   3],\n",
       "       [  7,   1,   1,   1,   3,   1,  14, 148,  30,  20,   5,   2,   8,\n",
       "          3,   3,   2,   7,   2,   4,   3],\n",
       "       [ 11,   0,   2,   3,   0,   0,  12,  16, 162,   9,   1,   1,   5,\n",
       "          1,   3,   2,   7,   4,   8,   4],\n",
       "       [  9,   2,   2,   0,   0,   1,   2,   5,  14, 149,  12,   2,   1,\n",
       "          2,   0,   4,   4,   5,   7,   5],\n",
       "       [ 10,   1,   2,   0,   1,   2,   3,   4,  13, 124,  57,   6,   1,\n",
       "          8,   1,   1,   6,   5,  12,   5],\n",
       "       [ 13,   4,   0,   4,   4,   1,   4,   1,   2,  11,   4, 152,  10,\n",
       "          0,   4,   4,  14,   3,  15,   7],\n",
       "       [  4,   8,   3,  16,  13,   1,  13,  10,   4,  10,   6,  14, 112,\n",
       "          5,   3,   1,   2,   0,   3,   1],\n",
       "       [ 20,   0,   1,   0,   2,   1,   3,  11,  20,   6,   1,   4,   8,\n",
       "        140,   5,  11,   3,   3,   6,   4],\n",
       "       [ 24,   7,   0,   0,   1,   3,   3,  14,  22,  14,   7,   4,   9,\n",
       "          4, 121,   3,   1,   2,  15,   2],\n",
       "       [ 32,   2,   0,   0,   1,   0,   0,   2,   1,   5,   1,   0,   1,\n",
       "          5,   1, 158,   2,   5,   6,  21],\n",
       "       [ 10,   0,   3,   0,   0,   0,   3,   5,   9,   8,   2,   6,   3,\n",
       "          0,   3,   6, 144,   1,  20,  11],\n",
       "       [ 32,   0,   1,   0,   1,   0,   0,   0,   2,   5,   3,   2,   3,\n",
       "          5,   0,   4,   6, 145,   9,   6],\n",
       "       [ 21,   0,   0,   0,   0,   0,   0,   4,   7,  12,   2,   2,   2,\n",
       "          9,   6,   6,  32,   1,  78,  15],\n",
       "       [ 29,   1,   1,   0,   0,   1,   1,   0,   1,   5,   1,   4,   1,\n",
       "          1,   1,  28,  12,   1,   1,  43]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix as cnf, classification_report as cr\n",
    "y_pred = clf.predict(XX)\n",
    "cnf(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d19ff",
   "metadata": {},
   "source": [
    "# OWN IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8af977b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.log_class_priors = None\n",
    "        self.log_class_word_freqs = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes, counts = np.unique(y, return_counts=True)\n",
    "        self.log_class_priors = np.log(counts) - np.log(np.sum(counts))\n",
    "        self.log_class_word_freqs = [np.log(X[y==c].sum(axis=0) + 1) - np.log(np.sum(X[y==c]) + X.shape[1]) for c in self.classes]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict_single_point(x) for x in X]\n",
    "\n",
    "    def _predict_single_point(self, x):\n",
    "        log_probs = [self.log_class_priors[i] + np.sum(x * self.log_class_word_freqs[i]) for i in range(len(self.classes))]\n",
    "        return self.classes[np.argmax(log_probs)]\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.fit(X, y_train)\n",
    "y_pred_own = nb.predict(XX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33a0218d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[119,   1,   1,   0,   0,   1,   1,   3,   7,   7,   0,   4,   1,\n",
       "          1,   0,  21,   5,   3,   3,  21],\n",
       "       [  4, 121,   1,  13,  14,  16,   6,   4,   6,   9,   5,  13,  12,\n",
       "          2,   9,   1,   3,   0,   0,   3],\n",
       "       [  7,  32,   5,  83,  21,  51,   2,   6,   5,   9,   2,  13,  13,\n",
       "          3,   1,   3,   1,   0,   5,   1],\n",
       "       [  3,   6,   0, 143,  49,   0,  14,   6,   0,   7,   6,   2,  19,\n",
       "          1,   1,   0,   0,   1,   3,   1],\n",
       "       [  1,   7,   2,  25, 126,   1,  16,   7,   5,   8,   2,   5,  22,\n",
       "          3,   1,   0,   1,   0,   1,   1],\n",
       "       [  1,  41,   0,   7,   5, 139,   8,   2,   0,   1,   3,   6,   5,\n",
       "          3,   3,   1,   1,   0,   2,   2],\n",
       "       [  0,   4,   1,  11,  11,   1, 160,  18,   6,   3,   6,   7,  20,\n",
       "          1,   3,   0,   0,   1,   1,   3],\n",
       "       [  7,   1,   1,   1,   3,   1,  14, 148,  30,  20,   5,   2,   8,\n",
       "          3,   3,   2,   7,   2,   4,   3],\n",
       "       [ 11,   0,   2,   3,   0,   0,  12,  16, 162,   9,   1,   1,   5,\n",
       "          1,   3,   2,   7,   4,   8,   4],\n",
       "       [  9,   2,   2,   0,   0,   1,   2,   5,  14, 149,  12,   2,   1,\n",
       "          2,   0,   4,   4,   5,   7,   5],\n",
       "       [ 10,   1,   2,   0,   1,   2,   3,   4,  13, 124,  57,   6,   1,\n",
       "          8,   1,   1,   6,   5,  12,   5],\n",
       "       [ 13,   4,   0,   4,   4,   1,   4,   1,   2,  11,   4, 152,  10,\n",
       "          0,   4,   4,  14,   3,  15,   7],\n",
       "       [  4,   8,   3,  16,  13,   1,  13,  10,   4,  10,   6,  14, 112,\n",
       "          5,   3,   1,   2,   0,   3,   1],\n",
       "       [ 20,   0,   1,   0,   2,   1,   3,  11,  20,   6,   1,   4,   8,\n",
       "        140,   5,  11,   3,   3,   6,   4],\n",
       "       [ 24,   7,   0,   0,   1,   3,   3,  14,  22,  14,   7,   4,   9,\n",
       "          4, 121,   3,   1,   2,  15,   2],\n",
       "       [ 32,   2,   0,   0,   1,   0,   0,   2,   1,   5,   1,   0,   1,\n",
       "          5,   1, 158,   2,   5,   6,  21],\n",
       "       [ 10,   0,   3,   0,   0,   0,   3,   5,   9,   8,   2,   6,   3,\n",
       "          0,   3,   6, 144,   1,  20,  11],\n",
       "       [ 32,   0,   1,   0,   1,   0,   0,   0,   2,   5,   3,   2,   3,\n",
       "          5,   0,   4,   6, 145,   9,   6],\n",
       "       [ 21,   0,   0,   0,   0,   0,   0,   4,   7,  12,   2,   2,   2,\n",
       "          9,   6,   6,  32,   1,  78,  15],\n",
       "       [ 29,   1,   1,   0,   0,   1,   1,   0,   1,   5,   1,   4,   1,\n",
       "          1,   1,  28,  12,   1,   1,  43]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf(y_test,y_pred_own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7ae43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644c511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
